doc_id tensor([[59235]])
my_Mean None
my_ReLU None
my_Conv2d None
my_ReLU None
my_Conv2d None
my_ReLU None
my_Conv2d None
=== conv mode1 ===
save input shape =  torch.Size([1, 300, 1, 1000])
weight shape =  torch.Size([500, 300, 1, 5])
out shape =  torch.Size([1, 500, 1, 1000])
out[>0] =  315298
out[<0] =  184702
out[0] =  tensor(-0.0086)
weight[>0] =  377654
weight[0,0] =  tensor([[ 0.0254,  0.0132, -0.0126,  0.0115,  0.0181]], requires_grad=True)
=== conv mode1 ===
save input shape =  torch.Size([1, 500, 1, 1000])
weight shape =  torch.Size([300, 500, 1, 1])
out shape =  torch.Size([1, 300, 1, 1000])
out[>0] =  255059
out[<0] =  44941
out[0] =  tensor(0.0815)
weight[>0] =  81716
weight[0,0] =  tensor([[0.0403]], requires_grad=True)
=== conv mode1 ===
save input shape =  torch.Size([1, 300, 1, 1000])
weight shape =  torch.Size([10000, 300, 1, 1])
out shape =  torch.Size([1, 10000, 1, 1000])
out[>0] =  280339
out[<0] =  9719661
out[0] =  tensor(-0.2307)
weight[>0] =  1435179
weight[0,0] =  tensor([[-0.0574]], requires_grad=True)
count(repr dimension > 0) =  4235
=== mean ebackwrd ===
pre_out shape = torch.Size([1, 10000, 1, 1000])
selp shape =  torch.Size([57])
prev shape =  torch.Size([57])
nrs shape  = torch.Size([3, 57])
channel size  = 1000
curv sum =  tensor(0.8068)
curv =  tensor([0.0071, 0.0072, 0.0078, 0.0080, 0.0081, 0.0081, 0.0085, 0.0085, 0.0090,
        0.0074, 0.0091, 0.0095, 0.0095, 0.0097, 0.0105, 0.0116, 0.0070, 0.0128,
        0.0144, 0.0145, 0.0146, 0.0146, 0.0093, 0.0095, 0.0156, 0.0123, 0.0160,
        0.0161, 0.0107, 0.0173, 0.0177, 0.0184, 0.0076, 0.0181, 0.0202, 0.0087,
        0.0130, 0.0159, 0.0203, 0.0142, 0.0204, 0.0204, 0.0206, 0.0113, 0.0207,
        0.0215, 0.0100, 0.0138, 0.0216, 0.0175, 0.0220, 0.0175, 0.0224, 0.0230,
        0.0187, 0.0231, 0.0239])
nrs =  tensor([[6.5321e+06, 6.5326e+06, 6.5326e+06, 6.5320e+06, 6.5320e+06, 6.5323e+06,
         6.5321e+06, 6.5324e+06, 6.5321e+06, 6.5327e+06, 6.5325e+06, 6.5329e+06,
         6.5325e+06, 6.5323e+06, 6.5329e+06, 6.5324e+06, 6.5328e+06, 6.5326e+06,
         6.5321e+06, 6.5323e+06, 6.5323e+06, 6.5325e+06, 6.5321e+06, 6.5330e+06,
         6.5322e+06, 6.5329e+06, 6.5329e+06, 6.5329e+06, 6.5321e+06, 6.5328e+06,
         6.5324e+06, 6.5322e+06, 6.5329e+06, 6.5325e+06, 6.5324e+06, 6.5323e+06,
         6.5321e+06, 6.5326e+06, 6.5322e+06, 6.5323e+06, 6.5328e+06, 6.5328e+06,
         6.5323e+06, 6.5321e+06, 6.5322e+06, 6.5328e+06, 6.5326e+06, 6.5330e+06,
         6.5323e+06, 6.5328e+06, 6.5323e+06, 6.5320e+06, 6.5324e+06, 6.5329e+06,
         6.5324e+06, 6.5328e+06, 6.5320e+06],
        [2.4631e-02, 2.5143e-02, 2.7307e-02, 2.8011e-02, 2.8091e-02, 2.8168e-02,
         2.9616e-02, 2.9643e-02, 3.1435e-02, 2.5725e-02, 3.1756e-02, 3.2944e-02,
         3.3063e-02, 3.3748e-02, 3.6588e-02, 4.0272e-02, 2.4523e-02, 4.4726e-02,
         5.0308e-02, 5.0399e-02, 5.0801e-02, 5.0917e-02, 3.2455e-02, 3.3126e-02,
         5.4389e-02, 4.2910e-02, 5.5804e-02, 5.6221e-02, 3.7224e-02, 6.0401e-02,
         6.1834e-02, 6.4267e-02, 2.6337e-02, 6.2980e-02, 7.0242e-02, 3.0395e-02,
         4.5480e-02, 5.5294e-02, 7.0911e-02, 4.9339e-02, 7.0982e-02, 7.1198e-02,
         7.1795e-02, 3.9414e-02, 7.1985e-02, 7.4761e-02, 3.4785e-02, 4.8095e-02,
         7.5196e-02, 6.0994e-02, 7.6715e-02, 6.1067e-02, 7.8230e-02, 8.0317e-02,
         6.5124e-02, 8.0520e-02, 8.3210e-02],
        [7.0672e-03, 7.2140e-03, 7.8349e-03, 8.0369e-03, 8.0599e-03, 8.0820e-03,
         8.4975e-03, 8.5052e-03, 9.0193e-03, 7.3809e-03, 9.1113e-03, 9.4524e-03,
         9.4864e-03, 9.6830e-03, 1.0498e-02, 1.1555e-02, 7.0361e-03, 1.2833e-02,
         1.4434e-02, 1.4461e-02, 1.4576e-02, 1.4609e-02, 9.3119e-03, 9.5045e-03,
         1.5605e-02, 1.2312e-02, 1.6011e-02, 1.6131e-02, 1.0680e-02, 1.7330e-02,
         1.7741e-02, 1.8439e-02, 7.5566e-03, 1.8070e-02, 2.0154e-02, 8.7208e-03,
         1.3049e-02, 1.5865e-02, 2.0346e-02, 1.4156e-02, 2.0366e-02, 2.0428e-02,
         2.0599e-02, 1.1309e-02, 2.0654e-02, 2.1450e-02, 9.9805e-03, 1.3799e-02,
         2.1575e-02, 1.7500e-02, 2.2011e-02, 1.7521e-02, 2.2446e-02, 2.3045e-02,
         1.8685e-02, 2.3103e-02, 2.3874e-02]])
=== Relu ebackwrd ===
!now nothing done. but highly recommend to check ReLU value is 1
=== Conv2d ebackward ===
pre_out shape = torch.Size([300000])
weight shape = torch.Size([10000, 300, 1, 1])
selp shape =  torch.Size([173])
nrs shape  = torch.Size([3, 173])
curv sum =  tensor(0.7410)
=== Relu ebackwrd ===
!now nothing done. but highly recommend to check ReLU value is 1
=== Conv2d ebackward ===
pre_out shape = torch.Size([500000])
weight shape = torch.Size([300, 500, 1, 1])
selp shape =  torch.Size([2202])
nrs shape  = torch.Size([3, 2202])
curv sum =  tensor(0.6018)
=== Relu ebackwrd ===
!now nothing done. but highly recommend to check ReLU value is 1
=== Conv2d ebackward ===
pre_out shape = torch.Size([300000])
weight shape = torch.Size([500, 300, 1, 5])
selp shape =  torch.Size([4858])
nrs shape  = torch.Size([3, 4858])
curv sum =  tensor(0.5129)
word sum nonzero count =  73
top 20 position= tensor([ 72,  73,  76, 343, 373, 776, 364, 757,  35, 758, 203, 202, 366,  36,
        990, 341, 191, 777, 389, 298])
top 20 value= tensor([0.1455, 0.1115, 0.0462, 0.0284, 0.0181, 0.0146, 0.0110, 0.0109, 0.0103,
        0.0088, 0.0086, 0.0086, 0.0079, 0.0076, 0.0072, 0.0069, 0.0067, 0.0066,
        0.0066, 0.0066])
top 20 count position= tensor([343, 191,  36, 366, 777, 311, 390, 201, 929, 756,  73,  48, 190, 969,
        928, 868, 293,  35, 202,  34])
top 20 count = tensor([84., 63., 59., 58., 55., 54., 53., 51., 50., 50., 50., 49., 49., 47.,
        47., 47., 46., 46., 46., 45.])
